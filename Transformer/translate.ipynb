{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import pdb\n",
    "import dill as pickle\n",
    "import argparse\n",
    "from Models import get_model\n",
    "from Beam import beam_search\n",
    "from nltk.corpus import wordnet\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from spm_tokenize import *\n",
    "from spm_vocab import *\n",
    "from spm_handler import *\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(vocab, tokenizer, sent):\n",
    "    tokens = tokenizer.tokenizer(sent)\n",
    "    token2idx = []\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        token2idx.append(vocab.stoi.get(token, vocab.unk_index))\n",
    "\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, opt, vocab, tokenizer):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    indexed = []\n",
    "    #sentence = SRC.preprocess(sentence)\n",
    "\n",
    "    sentence = convert_tokens_to_ids(vocab, tokenizer, sentence)\n",
    "\n",
    "    sentence = Variable(torch.LongTensor([sentence]))\n",
    "    sentence = sentence.cuda()\n",
    "    \n",
    "    sentence = beam_search(sentence, model, vocab, opt)\n",
    "\n",
    "    result = [multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sent) for sent in sentence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(opt, model, vocab, tokenizer, sentences):\n",
    "    sentences = sentences.lower().split('.')\n",
    "    #sentences = sentences.lower()\n",
    "    translated = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        result = translate_sentence(sentence + '.', model, opt, vocab, tokenizer)\n",
    "        translated = [each.capitalize() for each in result]\n",
    "\n",
    "    \n",
    "    result = [tokenizer.tokenize.DecodePieces(sent.split(' ')) for sent in translated]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant:\n",
    "    def __init__(self):\n",
    "        self.load_weights = 'checkpoint'\n",
    "        self.k = 5\n",
    "        self.max_len = 32\n",
    "        self.d_model = 512\n",
    "        self.n_layers = 6\n",
    "        self.heads = 8\n",
    "        self.dropout = 0.1\n",
    "        self.no_cuda = 'store_true'\n",
    "        self.floyd = 'store_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Constant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Tokenizer and Vocab...\n"
     ]
    }
   ],
   "source": [
    "print(f'Load Tokenizer and Vocab...')\n",
    "sp_tokenizer = Tokenizer(is_train=False, model_prefix='spm')\n",
    "sp_vocab = sp_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 12,
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the extended vocab...\n"
     ]
    }
   ],
   "source": [
    "print(f'Load the extended vocab...')\n",
    "vocab = Vocabulary.load_vocab('./data/vocab')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 13,
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 32003)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 13,
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp_vocab), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.stoi['<pad>'] = 0\n",
    "vocab.stoi['<unk>'] = 1\n",
    "vocab.stoi['<sos>'] = 2\n",
    "vocab.stoi['<eos>'] = 3"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################TEST DATA######################\n",
    "# fitting the test dataset dir\n",
    "test_data_dir = ['./data/test/newstest2014_en', './data/test/newstest2014_de']\n",
    "test_dataset = Our_Handler(src_path=test_data_dir[0], \n",
    "                        tgt_path=test_data_dir[1],\n",
    "                        vocab=vocab, \n",
    "                        tokenizer=sp_tokenizer,\n",
    "                        max_len=256,\n",
    "                        is_test=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=8,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)\n",
    "opt.test = test_dataloader\n",
    "opt.test_len = len(test_dataloader)\n",
    "opt.load_weights = './32len_32batch_10epochs'\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 26,
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained weights...\n"
     ]
    }
   ],
   "source": [
    "model = get_model(opt, len(vocab), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "opt.src_pad = opt.trg_pad = vocab.pad_index\n",
    "\n",
    "test_loss = 0.\n",
    "test_ppl = 0.\n",
    "for batch_idx, (enc_input, dec_input, dec_output) in enumerate(opt.test):\n",
    "        \n",
    "    enc_input = enc_input.transpose(0,1).to(opt.device)\n",
    "    dec_input = dec_input.transpose(0,1).to(opt.device)\n",
    "    dec_output = dec_output.to(opt.device)\n",
    "\n",
    "    src_mask, trg_mask = create_masks(enc_input, dec_input, opt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(enc_input, dec_input, src_mask, trg_mask)\n",
    "\n",
    "    ys = dec_output.contiguous().view(-1)\n",
    "\n",
    "    loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    test_ppl += np.exp(loss.item())\n",
    "    #phrase = translate(opt, model, vocab, sp_tokenizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
=======
   "execution_count": 29,
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.765820864359538 6803.876554308888\n"
     ]
    }
   ],
   "source": [
    "avg_test_loss = test_loss/len(opt.test)\n",
    "avg_ppl = test_ppl/len(opt.test)\n",
    "print(avg_test_loss, avg_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pc\n",
    "\n",
    "with open(test_data_dir[0], 'rb') as f:\n",
    "    test_data = pc.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gutach: Increased safety for pedestrians'], ['They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.'], ['Two sets of lights so close to one another: intentional or just a silly error?'], [\"Yesterday, Gutacht's Mayor gave a clear answer to this question.\"], ['\"At the time, the Town Hall traffic lights were installed because this was a school route,\" explained Eckert yesterday.']]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_result = []\n",
    "for each in test_data:\n",
    "    translated_result.append(translate(opt, model, vocab, sp_tokenizer, each[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3003 3003\n"
=======
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-72e0a7f79342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error opening or reading text file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3017dccb4b4d>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(opt, model, vocab, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtranslated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-590a0f090543>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(sentence, model, opt, vocab, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m  \u001b[0mmultiple_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m' ?'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' !'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' .'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'\\''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' ,'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/Diversifying_NMT_Transformer/Transformer/Beam.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(src, model, vocab, opt)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0meos_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
>>>>>>> 1d4b708d71c71f3db6fce42ba82b65cbc9a27c65
     ]
    }
   ],
   "source": [
    "print(len(translated_result), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On the occasion of World vegan Day on 1 November, the Vegetarian Association of Germany will propose a host of vegan alternatives:']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[723]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. ist▁die,,', 'ich ist▁die,,', 'die ist▁die,,', '.▁ist,,,', 'ich▁ist,,,']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 ['July.']\n",
      "1155 ['Killing.']\n"
     ]
    }
   ],
   "source": [
    "for idx, each in enumerate(test_data):\n",
    "    if len(each[0]) <= 10:\n",
    "        print(idx, each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die präsident, herr kommissar',\n",
       " 'die präsident, herr kommiss',\n",
       " 'die präsident, ich möchte',\n",
       " 'die präsident, herr kommissarar',\n",
       " 'die präsident, dass die']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_result[723]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die präsident, herr kommissarar 723\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for idx, each in enumerate(translated_result):\n",
    "    for ele in each:\n",
    "        if len(ele) == 31:\n",
    "            print(ele, idx)\n",
    "        length.append(len(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_data_dir[1], 'rb') as f:\n",
    "    refer_data = pc.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def bleu_compute(reference, translation):\n",
    "    \n",
    "    reference = reference.split()\n",
    "    translation = translation.split()\n",
    "    return corpus_bleu([reference], [translation], smoothing_function=SmoothingFunction().method7, weights=[1./3, 1./3, 1./3])\n",
    "\n",
    "def rfb_compute(bleu_score_list):\n",
    "    return sum(bleu_score_list) / len(bleu_score_list)\n",
    "\n",
    "def pwb_compute(translation_list):\n",
    "    pwb_score = []\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            #print(translation_list[i])\n",
    "            #print(translation_list[j])\n",
    "            pwb_score.append(bleu_compute(translation_list[i], translation_list[j]))\n",
    "            #print(f\"{i}/{j}---------------\")\n",
    "    assert len(pwb_score) == 20\n",
    "    \n",
    "    return sum(pwb_score) / len(pwb_score)\n",
    "\n",
    "def deq_compute(rfb, rfb_base, pwb, pwb_base):\n",
    "    return (pwb_base - pwb) / (rfb_base - rfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfb_total = 0.\n",
    "pwb_total = 0.\n",
    "for idx in range(len(refer_data)):\n",
    "    temp_bleu = []\n",
    "    for each in translated_result[idx]:\n",
    "        temp_bleu.append(bleu_compute(refer_data[idx][0], each))\n",
    "        #temp_bleu.append()\n",
    "    rfb_result = rfb_compute(temp_bleu)\n",
    "    pwb_result = pwb_compute(translated_result[idx])\n",
    "    \n",
    "    rfb_total += rfb_result\n",
    "    pwb_total += pwb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152.87771607269048, 2.1721009832796456)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfb_total, pwb_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfb 0.05090833036053629 pwb 0.0007233103507424727\n"
     ]
    }
   ],
   "source": [
    "print(f'rfb {rfb_total/len(refer_data)} pwb {pwb_total/len(refer_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love you']\n",
      "ich möchte▁ich,,\n",
      "\n",
      "ich ist▁die,,\n",
      "\n",
      "ich möchte▁die,,\n",
      "\n",
      "ich ist▁ich,,\n",
      "\n",
      "ich▁ist,,,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#opt.text =input(\"Enter a sentence to translate (type 'f' to load from file, or 'q' to quit):\\n\")\n",
    "opt.text = 'I love you'\n",
    "\n",
    "phrase = translate(opt, model, vocab, sp_tokenizer, opt.text)\n",
    "for each in phrase:\n",
    "    print(each + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
