{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from Models import get_model\n",
    "from Process import *\n",
    "import torch.nn.functional as F\n",
    "from Optim import CosineWithRestarts\n",
    "from Batch import create_masks\n",
    "import pdb\n",
    "import dill as pickle\n",
    "import argparse\n",
    "from Models import get_model\n",
    "from Beam import beam_search\n",
    "from nltk.corpus import wordnet\n",
    "from torch.autograd import Variable\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(vocab, tokenizer, sent):\n",
    "    tokens = tokenizer.tokenizer(sent)\n",
    "    token2idx = []\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        token2idx.append(vocab.stoi.get(token, vocab.unk_index))\n",
    "\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, opt, vocab, tokenizer):\n",
    "    \n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    #sentence = SRC.preprocess(sentence)\n",
    "    print(sentence)\n",
    "    sentence = convert_tokens_to_ids(vocab, tokenizer, sentence)\n",
    "\n",
    "    sentence = Variable(torch.LongTensor([sentence]))\n",
    "    \n",
    "    sentence = sentence.cuda()\n",
    "    \n",
    "    sentence = beam_search(sentence, model, vocab, opt)\n",
    "\n",
    "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(opt, model, vocab, tokenizer):\n",
    "    sentences = opt.text.lower().split('.')\n",
    "    translated = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        translated.append(translate_sentence(sentence + '.', model, opt, vocab, tokenizer).capitalize())\n",
    "\n",
    "    return (' '.join(translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-load_weights', required=True)\n",
    "parser.add_argument('-k', type=int, default=3)\n",
    "parser.add_argument('-max_len', type=int, default=80)\n",
    "parser.add_argument('-d_model', type=int, default=512)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Load Tokenizer and Vocab...')\n",
    "sp_tokenizer = Tokenizer(is_train=False, model_prefix='spm')\n",
    "sp_vocab = sp_tokenizer.vocab\n",
    "sp_proc = sp_tokenizer.tokenize\n",
    "\n",
    "print(f'Load the extended vocab...')\n",
    "vocab = Vocabulary.load_vocab('./data/vocab')\n",
    "\n",
    "######################TEST DATA######################\n",
    "# fitting the test dataset dir\n",
    "test_data_dir = ['./data/test/newstest2014_en', './data/test/newstest2014_de']\n",
    "test_dataset = Our_Handler(src_path=test_data_dir[0], \n",
    "                        tgt_path=test_data_dir[1],\n",
    "                        vocab=vocab, \n",
    "                        tokenizer=sp_tokenizer,\n",
    "                        max_len=256,\n",
    "                        is_test=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=8,\n",
    "                        shuffle=False,\n",
    "                        drop_last=True)\n",
    "opt.test = test_dataloader\n",
    "opt.test_len = len(test_dataloader)\n",
    "####################################################\n",
    "model = get_model(opt, len(vocab), len(vocab))\n",
    "\n",
    "\n",
    "phrase = translate(opt, model, vocab, sp_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
